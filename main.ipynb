{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.label import classes\n",
    "from helper.audio_extraction.get_file_list import get_file_list\n",
    "from helper.audio_extraction.padded_and_windowed import extract_windowed_features\n",
    "from FeedForward import ChordAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pribadi\\Chord Guitar AI\\venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "d:\\pribadi\\Chord Guitar AI\\venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=1024 is too large for input signal of length=345\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windowed_features:  936\n",
      "test_windowed_features:  240\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_PATH = './audio'\n",
    "train_list = get_file_list(TRAINING_DATA_PATH)\n",
    "data, train_data = extract_windowed_features(train_list, classes, test_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = data[0][0].shape[0]\n",
    "OUTPUT_SIZE = len(classes)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ChordAI(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_data_loader(data, BATCH_SIZE)\n",
    "test_dataloader = create_data_loader(train_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_function, optimizer, device, mode='train'):\n",
    "    acc = 0\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # change to tensor\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        # calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(predictions, targets)\n",
    "        if mode == 'train':\n",
    "            # backpropagate error and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # calculate accuracy\n",
    "        acc += (predictions.argmax(1) == targets).sum().item()\n",
    "    return acc / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, test_data, loss_function, optimizer, device):\n",
    "    tac, tsc = [], []\n",
    "    i = 0\n",
    "    patience = 0\n",
    "    scr = 0\n",
    "    start_time = datetime.now()\n",
    "    while True:\n",
    "        i+=1\n",
    "        print(f\"\\nEpoch : {i:4} | \", end=\" \")\n",
    "        train_acc = train_one_epoch(model, train_data, loss_function, optimizer, device)\n",
    "        tac.append(train_acc)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_acc = train_one_epoch(model, test_data, loss_function, optimizer, device, mode='test')\n",
    "            tsc.append(test_acc)\n",
    "\n",
    "        print(f\"train acc : {train_acc:.4f} | test acc : {test_acc:.4f} | patience : {patience} | best acc : {scr:.4f}\", end=\" \") \n",
    "        if test_acc > scr:\n",
    "            scr = test_acc\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), \"models/chord_model.pth\")\n",
    "            log = {\n",
    "                \"train_acc\": tac,\n",
    "                \"test_acc\": tsc\n",
    "            }\n",
    "            torch.save(log, \"models/logs.pth\")\n",
    "        else:\n",
    "            patience +=1\n",
    "\n",
    "        if patience >= 5:\n",
    "            break\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nTraining completed in {(end_time-start_time).seconds} seconds\")\n",
    "    torch.save({\n",
    "        'INPUT_SIZE': INPUT_SIZE,\n",
    "        'OUTPUT_SIZE': OUTPUT_SIZE,\n",
    "    }, \"models/config.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch :    1 |  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arwan\\AppData\\Local\\Temp\\ipykernel_312468\\1042002776.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "C:\\Users\\arwan\\AppData\\Local\\Temp\\ipykernel_312468\\1042002776.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.0085 | test acc : 0.2042 | patience : 0 | best acc : 0.0000 \n",
      "Epoch :    2 |  train acc : 0.1677 | test acc : 0.3708 | patience : 0 | best acc : 0.2042 \n",
      "Epoch :    3 |  train acc : 0.3237 | test acc : 0.4417 | patience : 0 | best acc : 0.3708 \n",
      "Epoch :    4 |  train acc : 0.4605 | test acc : 0.5792 | patience : 0 | best acc : 0.4417 \n",
      "Epoch :    5 |  train acc : 0.6218 | test acc : 0.6458 | patience : 0 | best acc : 0.5792 \n",
      "Epoch :    6 |  train acc : 0.6699 | test acc : 0.6375 | patience : 0 | best acc : 0.6458 \n",
      "Epoch :    7 |  train acc : 0.7297 | test acc : 0.7375 | patience : 1 | best acc : 0.6458 \n",
      "Epoch :    8 |  train acc : 0.7767 | test acc : 0.7125 | patience : 0 | best acc : 0.7375 \n",
      "Epoch :    9 |  train acc : 0.8697 | test acc : 0.8167 | patience : 1 | best acc : 0.7375 \n",
      "Epoch :   10 |  train acc : 0.8974 | test acc : 0.8042 | patience : 0 | best acc : 0.8167 \n",
      "Epoch :   11 |  train acc : 0.9028 | test acc : 0.8167 | patience : 1 | best acc : 0.8167 \n",
      "Epoch :   12 |  train acc : 0.9284 | test acc : 0.8333 | patience : 2 | best acc : 0.8167 \n",
      "Epoch :   13 |  train acc : 0.9060 | test acc : 0.8458 | patience : 0 | best acc : 0.8333 \n",
      "Epoch :   14 |  train acc : 0.9348 | test acc : 0.8583 | patience : 0 | best acc : 0.8458 \n",
      "Epoch :   15 |  train acc : 0.9380 | test acc : 0.8625 | patience : 0 | best acc : 0.8583 \n",
      "Epoch :   16 |  train acc : 0.9241 | test acc : 0.8583 | patience : 0 | best acc : 0.8625 \n",
      "Epoch :   17 |  train acc : 0.9359 | test acc : 0.8625 | patience : 1 | best acc : 0.8625 \n",
      "Epoch :   18 |  train acc : 0.9391 | test acc : 0.8667 | patience : 2 | best acc : 0.8625 \n",
      "Epoch :   19 |  train acc : 0.9391 | test acc : 0.8750 | patience : 0 | best acc : 0.8667 \n",
      "Epoch :   20 |  train acc : 0.9434 | test acc : 0.8625 | patience : 0 | best acc : 0.8750 \n",
      "Epoch :   21 |  train acc : 0.9455 | test acc : 0.8708 | patience : 1 | best acc : 0.8750 \n",
      "Epoch :   22 |  train acc : 0.9124 | test acc : 0.8625 | patience : 2 | best acc : 0.8750 \n",
      "Epoch :   23 |  train acc : 0.9434 | test acc : 0.8792 | patience : 3 | best acc : 0.8750 \n",
      "Epoch :   24 |  train acc : 0.9530 | test acc : 0.9000 | patience : 0 | best acc : 0.8792 \n",
      "Epoch :   25 |  train acc : 0.9562 | test acc : 0.9208 | patience : 0 | best acc : 0.9000 \n",
      "Epoch :   26 |  train acc : 0.9573 | test acc : 0.9208 | patience : 0 | best acc : 0.9208 \n",
      "Epoch :   27 |  train acc : 0.9391 | test acc : 0.9375 | patience : 1 | best acc : 0.9208 \n",
      "Epoch :   28 |  train acc : 0.9551 | test acc : 0.9250 | patience : 0 | best acc : 0.9375 \n",
      "Epoch :   29 |  train acc : 0.9594 | test acc : 0.9375 | patience : 1 | best acc : 0.9375 \n",
      "Epoch :   30 |  train acc : 0.9615 | test acc : 0.9292 | patience : 2 | best acc : 0.9375 \n",
      "Epoch :   31 |  train acc : 0.9615 | test acc : 0.9542 | patience : 3 | best acc : 0.9375 \n",
      "Epoch :   32 |  train acc : 0.9658 | test acc : 0.9292 | patience : 0 | best acc : 0.9542 \n",
      "Epoch :   33 |  train acc : 0.9583 | test acc : 0.9500 | patience : 1 | best acc : 0.9542 \n",
      "Epoch :   34 |  train acc : 0.9605 | test acc : 0.9500 | patience : 2 | best acc : 0.9542 \n",
      "Epoch :   35 |  train acc : 0.9615 | test acc : 0.9542 | patience : 3 | best acc : 0.9542 \n",
      "Epoch :   36 |  train acc : 0.9882 | test acc : 0.9667 | patience : 4 | best acc : 0.9542 \n",
      "Epoch :   37 |  train acc : 0.9968 | test acc : 0.9542 | patience : 0 | best acc : 0.9667 \n",
      "Epoch :   38 |  train acc : 0.9989 | test acc : 0.9542 | patience : 1 | best acc : 0.9667 \n",
      "Epoch :   39 |  train acc : 0.9989 | test acc : 0.9542 | patience : 2 | best acc : 0.9667 \n",
      "Epoch :   40 |  train acc : 0.9989 | test acc : 0.9625 | patience : 3 | best acc : 0.9667 \n",
      "Epoch :   41 |  train acc : 0.9989 | test acc : 0.9583 | patience : 4 | best acc : 0.9667 \n",
      "Training completed in 7 seconds\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, test_dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
