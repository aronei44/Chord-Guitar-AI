{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.label import classes\n",
    "from helper.audio_extraction.get_file_list import get_file_list\n",
    "from helper.audio_extraction.padded_and_windowed import extract_windowed_features\n",
    "from FeedForward import ChordAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pribadi\\Chord Guitar AI\\venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=1024 is too large for input signal of length=690\n",
      "  warnings.warn(\n",
      "d:\\pribadi\\Chord Guitar AI\\venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=1024 is too large for input signal of length=345\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA_PATH = './audio'\n",
    "train_list = get_file_list(TRAINING_DATA_PATH)\n",
    "data = extract_windowed_features(train_list, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = data[0][0].shape[0]\n",
    "OUTPUT_SIZE = len(classes)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ChordAI(INPUT_SIZE, OUTPUT_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, batch_size):\n",
    "    dataloader = DataLoader(data, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_data_loader(data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_function, optimizer, device):\n",
    "    acc = 0\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # change to tensor\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        # calculate loss\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(predictions, targets)\n",
    "        # backpropagate error and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        acc += (predictions.argmax(1) == targets).sum().item()\n",
    "    return acc / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, loss_function, optimizer, device):\n",
    "    tac = []\n",
    "    i = 0\n",
    "    patience = 0\n",
    "    scr = 0\n",
    "    start_time = datetime.now()\n",
    "    while True:\n",
    "        i+=1\n",
    "        print(f\"\\nEpoch : {i:4} | \", end=\" \")\n",
    "        train_acc = train_one_epoch(model, train_data, loss_function, optimizer, device)\n",
    "        tac.append(train_acc)\n",
    "\n",
    "        print(f\"train acc : {train_acc:.4f} | patience : {patience} | best acc : {scr:.4f}\", end=\" \") \n",
    "        if train_acc > scr:\n",
    "            scr = train_acc\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), \"models/chord_model.pth\")\n",
    "            log = {\n",
    "            \"train_acc\": tac,\n",
    "            }\n",
    "            torch.save(log, \"models/logs.pth\")\n",
    "        else:\n",
    "            patience +=1\n",
    "\n",
    "        if patience >= 5:\n",
    "            break\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\nTraining completed in {(end_time-start_time).seconds} seconds\")\n",
    "    torch.save({\n",
    "        'INPUT_SIZE': INPUT_SIZE,\n",
    "        'OUTPUT_SIZE': OUTPUT_SIZE,\n",
    "    }, \"models/config.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch :    1 |  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arwan\\AppData\\Local\\Temp\\ipykernel_237760\\3847291912.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs, dtype=torch.float32)\n",
      "C:\\Users\\arwan\\AppData\\Local\\Temp\\ipykernel_237760\\3847291912.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc : 0.0145 | patience : 0 | best acc : 0.0000 \n",
      "Epoch :    2 |  train acc : 0.0884 | patience : 0 | best acc : 0.0145 \n",
      "Epoch :    3 |  train acc : 0.1684 | patience : 0 | best acc : 0.0884 \n",
      "Epoch :    4 |  train acc : 0.3844 | patience : 0 | best acc : 0.1684 \n",
      "Epoch :    5 |  train acc : 0.3912 | patience : 0 | best acc : 0.3844 \n",
      "Epoch :    6 |  train acc : 0.5680 | patience : 0 | best acc : 0.3912 \n",
      "Epoch :    7 |  train acc : 0.5723 | patience : 0 | best acc : 0.5680 \n",
      "Epoch :    8 |  train acc : 0.6692 | patience : 0 | best acc : 0.5723 \n",
      "Epoch :    9 |  train acc : 0.7976 | patience : 0 | best acc : 0.6692 \n",
      "Epoch :   10 |  train acc : 0.8520 | patience : 0 | best acc : 0.7976 \n",
      "Epoch :   11 |  train acc : 0.8673 | patience : 0 | best acc : 0.8520 \n",
      "Epoch :   12 |  train acc : 0.9022 | patience : 0 | best acc : 0.8673 \n",
      "Epoch :   13 |  train acc : 0.8971 | patience : 0 | best acc : 0.9022 \n",
      "Epoch :   14 |  train acc : 0.8912 | patience : 1 | best acc : 0.9022 \n",
      "Epoch :   15 |  train acc : 0.8597 | patience : 2 | best acc : 0.9022 \n",
      "Epoch :   16 |  train acc : 0.9269 | patience : 3 | best acc : 0.9022 \n",
      "Epoch :   17 |  train acc : 0.8937 | patience : 0 | best acc : 0.9269 \n",
      "Epoch :   18 |  train acc : 0.9175 | patience : 1 | best acc : 0.9269 \n",
      "Epoch :   19 |  train acc : 0.9430 | patience : 2 | best acc : 0.9269 \n",
      "Epoch :   20 |  train acc : 0.9328 | patience : 0 | best acc : 0.9430 \n",
      "Epoch :   21 |  train acc : 0.9583 | patience : 1 | best acc : 0.9430 \n",
      "Epoch :   22 |  train acc : 0.9702 | patience : 0 | best acc : 0.9583 \n",
      "Epoch :   23 |  train acc : 0.9549 | patience : 0 | best acc : 0.9702 \n",
      "Epoch :   24 |  train acc : 0.9269 | patience : 1 | best acc : 0.9702 \n",
      "Epoch :   25 |  train acc : 0.9354 | patience : 2 | best acc : 0.9702 \n",
      "Epoch :   26 |  train acc : 0.9821 | patience : 3 | best acc : 0.9702 \n",
      "Epoch :   27 |  train acc : 0.9957 | patience : 0 | best acc : 0.9821 \n",
      "Epoch :   28 |  train acc : 0.9957 | patience : 0 | best acc : 0.9957 \n",
      "Epoch :   29 |  train acc : 0.9762 | patience : 1 | best acc : 0.9957 \n",
      "Epoch :   30 |  train acc : 0.9889 | patience : 2 | best acc : 0.9957 \n",
      "Epoch :   31 |  train acc : 0.9983 | patience : 3 | best acc : 0.9957 \n",
      "Epoch :   32 |  train acc : 0.9906 | patience : 0 | best acc : 0.9983 \n",
      "Epoch :   33 |  train acc : 1.0000 | patience : 1 | best acc : 0.9983 \n",
      "Epoch :   34 |  train acc : 0.9906 | patience : 0 | best acc : 1.0000 \n",
      "Epoch :   35 |  train acc : 0.9991 | patience : 1 | best acc : 1.0000 \n",
      "Epoch :   36 |  train acc : 1.0000 | patience : 2 | best acc : 1.0000 \n",
      "Epoch :   37 |  train acc : 1.0000 | patience : 3 | best acc : 1.0000 \n",
      "Epoch :   38 |  train acc : 1.0000 | patience : 4 | best acc : 1.0000 \n",
      "Training completed in 3 seconds\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
